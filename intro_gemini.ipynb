{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"qwiklabs-gcp-01-906614e3d355\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-east4\"  # @param {type:\"string\"}\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerationConfig, GenerativeModel, Image, Part\n",
    "# importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "# loading gemini 1.0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = model.generate_content(\"Why is the sky blue?\", stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n",
    "\n",
    "# generating content using gemini 1.0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Create a numbered list of 10 items. Each item in the list should be a trend in the tech industry.\n",
    "\n",
    "Each trend should be less than 5 words.\"\"\"  # try your own prompt\n",
    "\n",
    "responses = model.generate_content(prompt, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.9,\n",
    "    top_p=1.0,\n",
    "    top_k=32,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=8192,\n",
    ")\n",
    "\n",
    "responses = model.generate_content(\n",
    "    \"Why is the sky blue?\",\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n",
    "\n",
    "    # model paramters: temperature, top_p, top_k, candidate_count, max_output_tokens\n",
    "    #Every prompt you send to the model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model.start_chat()\n",
    "\n",
    "prompt = \"\"\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\n",
    "\n",
    "Suggest another movie I might like.\n",
    "\"\"\"\n",
    "\n",
    "responses = chat.send_message(prompt, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Are my favorite movies based on a book series?\"\n",
    "\n",
    "responses = chat.send_message(prompt, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    for image in images:\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "        if pil_image.mode != \"RGB\":\n",
    "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "        image_width, image_height = pil_image.size\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            # Resize to display a smaller notebook image\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def get_url_from_gcs(gcs_uri: str) -> str:\n",
    "    # converts gcs uri to url for image display.\n",
    "    url = \"https://storage.googleapis.com/\" + gcs_uri.replace(\"gs://\", \"\").replace(\n",
    "        \" \", \"%20\"\n",
    "    )\n",
    "    return url\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list):\n",
    "    \"\"\"\n",
    "    Given contents that would be sent to Gemini,\n",
    "    output the full multimodal prompt for ease of readability.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        if isinstance(content, Image):\n",
    "            display_images([content])\n",
    "        elif isinstance(content, Part):\n",
    "            url = get_url_from_gcs(content.file_data.file_uri)\n",
    "            IPython.display.display(load_image_from_url(url))\n",
    "        else:\n",
    "            print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an image from Google Cloud Storage\n",
    "! gsutil cp \"gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg\" ./image.jpg\n",
    "\n",
    "# Load from local file\n",
    "image = Image.load_from_file(\"image.jpg\")\n",
    "\n",
    "# Prepare contents\n",
    "prompt = \"Describe this image?\"\n",
    "contents = [image, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image from Cloud Storage URI\n",
    "gcs_uri = \"gs://cloud-samples-data/generative-ai/image/boats.jpeg\"\n",
    "\n",
    "# Prepare contents\n",
    "image = Part.from_uri(gcs_uri, mime_type=\"image/jpeg\")\n",
    "prompt = \"Describe the scene?\"\n",
    "contents = [image, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image from Cloud Storage URI\n",
    "image_url = (\n",
    "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/boats.jpeg\"\n",
    ")\n",
    "image = load_image_from_url(image_url)  # convert to bytes\n",
    "\n",
    "# Prepare contents\n",
    "prompt = \"Describe the scene?\"\n",
    "contents = [image, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from Cloud Storage URI\n",
    "image1_url = \"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark1.jpg\"\n",
    "image2_url = \"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark2.jpg\"\n",
    "image3_url = \"https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg\"\n",
    "image1 = load_image_from_url(image1_url)\n",
    "image2 = load_image_from_url(image2_url)\n",
    "image3 = load_image_from_url(image3_url)\n",
    "\n",
    "# Prepare prompts\n",
    "prompt1 = \"\"\"{\"city\": \"London\", \"Landmark:\", \"Big Ben\"}\"\"\"\n",
    "prompt2 = \"\"\"{\"city\": \"Paris\", \"Landmark:\", \"Eiffel Tower\"}\"\"\"\n",
    "\n",
    "# Prepare contents\n",
    "contents = [image1, prompt1, image2, prompt2, image3]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\"\n",
    "video_uri = f\"gs://{file_path}\"\n",
    "video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "\n",
    "IPython.display.Video(video_url, width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "What is the profession of the main person?\n",
    "What are the main features of the phone highlighted?\n",
    "Which city was this recorded in?\n",
    "Provide the answer JSON.\n",
    "\"\"\"\n",
    "\n",
    "video = Part.from_uri(video_uri, mime_type=\"video/mp4\")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
